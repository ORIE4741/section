{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#B31B1B'> Scikit-Learn </font>\n",
    "We've already seen scikit-learn in action in class as a tool for fitting different machine learning models. But scikit-learn has even more to offer! In this section we'll review some basic syntax for fitting models in scikit-learn and explore tools for data pre-processing and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#B31B1B'> Toy Data </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a custom 'toy' dataset that contains a nice mix of different data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/data_processing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is going to be to predict col3, which is a column with real valued features. This makes it **what kind of machine learning problem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#B31B1B'> Pre-Processing Data </font>\n",
    "For models like linear regression, we need all of our features to be numeric and not have any missing values. Luckily we've already seen some methods in class and homework to convert non-numeric features to a nice form. In scikit-learn we can connect all the steps to pre-process data into one operation called a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, a possible pipeline for our synthetic data could be:\n",
    "\n",
    "![pipeline](pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to build this pipeline using scikit learn's [pipelines](http://scikit-learn.org/stable/modules/pipeline.html).\n",
    "\n",
    "Scikit-learn [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) allows us to plug in together any number of transformers (that transform data) and estimators (that predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's grab our different data types\n",
    "target_variable = \"col3\"\n",
    "numerical_cols =  data.drop(columns=target_variable).select_dtypes(np.number).columns\n",
    "categorical_col = ['categorical_col']\n",
    "ordinal_col = [\"ordinal_col\"]\n",
    "text_col = ['text_col']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#B31B1B'> Numerical Pre-processing </font>\n",
    "\n",
    "We will create a pipeline for each kind of data first, then we will see how to put them together into a single Pipeline\n",
    "\n",
    "First we can start by pre-processing our numeric data. Some steps we might want to include\n",
    "\n",
    "- **Imputation**: Filling in missing values \n",
    "\n",
    "- **Scaling**: Normalizing our values (some options are max-min scaling, or subtracting mean and dividing by variance/standard deviation)\n",
    "\n",
    "We can access both of these operations in the scikit-learn impute and preprocessing modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"mean\") #Can set other strategies (check the help doc!)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to run these alone (not in a pipeline), the general syntax is 'fit' (fitting parameters for transformation, like mean/median/mode) then 'transform' (applying transform with fitted parameters). Let's take a look at applying the imputer to the 'missing_col'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before imputation\n",
    "data[['missing_col']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using imputation\n",
    "\n",
    "imputed_col = imputer.fit(data[['missing_col']]).transform(data[['missing_col']])\n",
    "#Or perform fit/transform together\n",
    "imputed_col = imputer.fit_transform(data[['missing_col']])\n",
    "\n",
    "pd.Series(imputed_col.reshape(-1)).isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where scikit-learn really shines is putting these operations together into a pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sklearn Pipeline is defined as a sequence of steps.\n",
    "\n",
    "For example, for the numerical variables our pipeline will have 2 steps:\n",
    "- first we will impute,\n",
    "- then we will scale. \n",
    "\n",
    "We can create this numerical pipeline like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_pipeline = make_pipeline(\n",
    "    imputer,\n",
    "    scaler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `fit` and `transform` with the pipeline and it will apply all the transformers sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_pipeline.fit_transform(data[numerical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is awesome, we dont need to concatenate the processing steps manually, but we still have a problem, we cant apply this numerical transformer to the whole dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_pipeline.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline fails when it tries to apply the imputer and the scaler to the non numerical columns.\n",
    "\n",
    "We can fix this by applying a step that will select only the numerical columns before applying the imputer and the scaler. We can use the package named  [mlxtend](https://github.com/rasbt/mlxtend/pull/378) (which provides additional functionalities to scikit-learn) to import a ColumnSelector, which is a transformer that selects columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for example, we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import ColumnSelector\n",
    "\n",
    "numerical_col_selector = ColumnSelector(cols=numerical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply this selector to the whole dataframe and it will automatically select the numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_col_selector.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a numerical pipeline that takes care of selection the appropriate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_pipeline = make_pipeline(\n",
    "    numerical_col_selector,\n",
    "    imputer,\n",
    "    scaler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can apply this pipeline to the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_pipeline.fit_transform(data)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### <font color='#B31B1B'> Text Pipeline </font>\n",
    "\n",
    "There are a number of ways to process text features. One of the most popular is TF-IDF (Term frequency-inverse document frequency). The high level idea is that we'll create one feature per word in our dataset, and the value for this new feature for each row is going to be the multiplication of its 'term-frequency' (i.e. how often the word occurs in the text column for this row) by the word's inverse document frequency (i.e. 1/the fraction of rows that have that word). \n",
    "\n",
    "To do this in scikit-learn we need a [DenseTransformer](https://rasbt.github.io/mlxtend/user_guide/preprocessing/DenseTransformer/) that transforms the output produced by TfidfVectorizer (a sparse matrix) into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from mlxtend.preprocessing import DenseTransformer\n",
    "\n",
    "text_pipeline = make_pipeline(\n",
    "    ColumnSelector(cols=text_col, drop_axis=True),\n",
    "    TfidfVectorizer(),\n",
    "    DenseTransformer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#B31B1B'> Categorical Pipeline </font>\n",
    "For categorical variables, we'll use **one-hot encoding**, which means we will create k binary columns (where k is the unique values for the variable). Then these columns will have all of their values 0 except one column that will represent the actual categorical value.\n",
    "\n",
    "We can use the transformer `OneHotEncoder` that is part of the package [`category_encoders`](http://contrib.scikit-learn.org/categorical-encoding/), which is a package that adds additional categorical encoders that are compatible with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same way as before, we will do One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_pipeline = make_pipeline(\n",
    "     ColumnSelector(cols=categorical_col),\n",
    "     OneHotEncoder()\n",
    ")\n",
    "\n",
    "categorical_pipeline.fit_transform(data)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#B31B1B'> Ordinal Pipeline </font>\n",
    "To encode the ordinal column we need to first, use column selector to select it and then use OrdinalEncoder. We need to remember that ordinal Encoder requires the selection of the columns we want to transform, and because the output of ColumnSelector is a numpy array, we have to specify the column in position 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import OrdinalEncoder\n",
    "\n",
    "# ColumnSelector's output is an array, so we use the column 0 for ordinal encoder\n",
    "ordinal_encoder = OrdinalEncoder(mapping=[\n",
    "    {\"col\": 0, \n",
    "      \"mapping\": {\n",
    "        \"very bad\": 0,\n",
    "        \"bad\": 1,\n",
    "        \"normal\": 2,\n",
    "        \"good\": 3,\n",
    "        \"very good\": 4\n",
    "      } \n",
    "     }\n",
    "])\n",
    "ordinal_pipeline = make_pipeline(\n",
    "    ColumnSelector(cols=ordinal_col),\n",
    "    ordinal_encoder\n",
    ")\n",
    "\n",
    "ordinal_pipeline.fit_transform(data)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#B31B1B'> Pipeline Union </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the individual pipelines that process each different datatype.\n",
    "\n",
    "Now we just need to get a [FeatureUnion](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html#sklearn.pipeline.FeatureUnion) to join all of the outputs together. A feature union doesnt apply all of its steps sequentially. Instead, it applies the input to all of the steps at once and puts them together\n",
    "\n",
    "![pipeline](pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_pipeline = make_union(\n",
    "    numerical_pipeline,\n",
    "    text_pipeline,\n",
    "    categorical_pipeline,\n",
    "    ordinal_pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this FeatureUnion to transform the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_data = processing_pipeline.fit_transform(data)\n",
    "\n",
    "fit_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we just have to add an estimator to the end of the pipeline, so it can be trained with the transformed data. Recall that the general syntax for training a model (outside of a pipeline) is fit then predict. For example, if we wanted to fit a linear regression on our pre-processed data we would:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(fit_data, data[target_variable])\n",
    "\n",
    "#Now our model's fit and we can either pull out the coefficients or use it to make predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can embed this learning process into our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LinearRegression()\n",
    "estimator_pipeline = make_pipeline(\n",
    "    processing_pipeline,\n",
    "    estimator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we added an estimator to the pipeline we can do `fit` and `predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_pipeline.fit(data.drop(target_variable, axis=1), data[target_variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_pipeline.predict(data)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pipelines not only helps organizing the different steps. We can also use all of scikit-learn tools with pipelines! For example, we can do cross validation with the estimator pipeline we just made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=target_variable)\n",
    "y = data[target_variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mae_cv = cross_val_score(estimator_pipeline, \n",
    "                X, \n",
    "                y,\n",
    "                scoring='neg_mean_absolute_error', \n",
    "                cv=5\n",
    ")\n",
    "mae_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_cv.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#B31B1B'> Hyperparameter Optimization </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because pipelines follow the scikit-learn convention of implementing the methods fit, transform and predict, we can use a search in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive_pipeline = Pipeline(\n",
    "    [\n",
    "     (\"processing\", processing_pipeline),\n",
    "     (\"estimator\", LinearRegression())   \n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline has a loot of hyperparams we can tweak, basically every single parameter on every single step of the pipeline, we  can see their names like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(predictive_pipeline.get_params().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll focus on whether or not we want to fit an intercept in our linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist_random = {\n",
    "    \"estimator__fit_intercept\": [True, False],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add hyperparameter tuning to our pipeline we simply set the estimator to be our predictive pipeline. There are a few ways to do automated hyperparameter selection in scikit-learn, RandomizedSearchCV and GridSearchCV. They both take a set of values to consider, but the first trys random selelctions of those hyperparameters, and the other looks at every possible combination. The second is thus guaranteed to do better but can take much longer (for those interested check out this <a href='https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf'> great paper </a> that compares the two.\n",
    "\n",
    "We'll use randomized search, but the syntax is the same for grid search (except you have to put in a set of values, not a distribution of potential values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_pipeline = RandomizedSearchCV(\n",
    "    estimator=predictive_pipeline, \n",
    "    param_distributions=param_dist_random,\n",
    "   scoring=\"neg_mean_squared_error\", n_jobs=-1, n_iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perform the randomized grid search for the whole pipeline (under the hood, we're trying a bunch of different hyperparameters then doing CV to get an estimate of its performance, we then return the one with the best CV performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "random_search_pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peak at our best model + score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random_search_pipeline.best_score_)\n",
    "print(random_search_pipeline.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even see what final steps we've applied in our best model (this is super handy if we're trying out different hyperparameters during data pre-processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_search_pipeline.best_estimator_.steps[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#B31B1B'> Exporting Pipelines </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The great thing about scikit learn pipelines is that we can save it, and reuse them **without having to retrain them**, we can use the library joblib to do so.\n",
    "\n",
    "For example, to save the pipeline we have built:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "dump(estimator_pipeline, 'pipeline.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded = load(\"pipeline.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can predict directly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reloaded.predict(X)[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
