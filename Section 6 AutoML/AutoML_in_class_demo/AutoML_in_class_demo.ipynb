{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Iris dataset to illustrate how different AutoML frameworks work, by doing model selection on the training set and then evaluate on test set. The error metric we are using is balanced error rate, which is the average of false positive rate and false negative rate, and then take the average of those averages across classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_path = 'oboe/automl/'\n",
    "sys.path.append(automl_path)\n",
    "from auto_learner import AutoLearner\n",
    "import util\n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataset: we would use either iris or Airbnb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"airbnb\" # airbnb or iris\n",
    "airbnb_dataset_size = 1000 # number of points to keep in subsampling\n",
    "\n",
    "if dataset == \"airbnb\":\n",
    "    df_airbnb = pd.read_csv(\"airbnb.csv\", index_col=None, header=0)\n",
    "    df_airbnb.drop(df_airbnb[df_airbnb.price == np.nan].index, inplace=True)\n",
    "    features_real = [\n",
    "      \"host_listings_count\",\n",
    "      \"host_total_listings_count\",\n",
    "      \"accommodates\",\n",
    "      \"bathrooms\",\n",
    "      \"bedrooms\",\n",
    "      \"guests_included\",\n",
    "      \"extra_people\",\n",
    "      \"minimum_nights\",\n",
    "      \"maximum_nights\",\n",
    "      \"availability_30\",\n",
    "      \"availability_60\",\n",
    "      \"availability_90\",\n",
    "      \"availability_365\",\n",
    "      \"number_of_reviews\",\n",
    "      \"review_scores_rating\",\n",
    "      \"review_scores_accuracy\",\n",
    "      \"review_scores_cleanliness\",\n",
    "      \"review_scores_checkin\",\n",
    "      \"review_scores_communication\",\n",
    "      \"review_scores_location\",\n",
    "      \"price\"\n",
    "    ]\n",
    "\n",
    "    label = [\"review_scores_value\"]\n",
    "    x = df_airbnb[features_real].values\n",
    "    y = df_airbnb[label].values.flatten()\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    idx_to_keep = np.random.choice(np.arange(y.shape[0]), size=airbnb_dataset_size, replace=False)\n",
    "    x = x[idx_to_keep]\n",
    "    y = y[idx_to_keep]\n",
    "    \n",
    "\n",
    "elif dataset == \"iris\":\n",
    "    data = load_iris()\n",
    "    x = np.array(data['data'])\n",
    "    y = np.array(data['target'])\n",
    "    \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: auto-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autosklearn.classification\n",
    "from autosklearn.metrics import balanced_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A wrapper class for the auto-sklearn learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AutoSklearn(total_runtime, train_features, train_labels):\n",
    "    clf = autosklearn.classification.AutoSklearnClassifier(\n",
    "            time_left_for_this_task=total_runtime,\n",
    "            include_preprocessors=[\"no_preprocessing\"],\n",
    "            include_estimators = [\"adaboost\",\"gaussian_nb\", \"extra_trees\", \"gradient_boosting\", \n",
    "                                 \"liblinear_svc\", \"libsvm_svc\",\"random_forest\",\n",
    "                                 \"k_nearest_neighbors\",\"decision_tree\"],\n",
    "    )\n",
    "        \n",
    "    clf.fit(train_features, train_labels, metric=balanced_accuracy)    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run auto-sklearn for 30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] [2019-10-30 15:00:15,081:EnsembleBuilder(1):9fd2013b7bf2c8c7e1ee2eb1f6d12a1a] No models better than random - using Dummy Score!\n",
      "[WARNING] [2019-10-30 15:00:15,103:EnsembleBuilder(1):9fd2013b7bf2c8c7e1ee2eb1f6d12a1a] No models better than random - using Dummy Score!\n",
      "Time limit for a single run is higher than total time limit. Capping the limit for a single run to the total time given to SMAC (29.593113)\n",
      "1\n",
      "['/tmp/autosklearn_tmp_64801_2482/.auto-sklearn/ensembles/1.0000000000.ensemble', '/tmp/autosklearn_tmp_64801_2482/.auto-sklearn/ensembles/1.0000000001.ensemble', '/tmp/autosklearn_tmp_64801_2482/.auto-sklearn/ensembles/1.0000000002.ensemble', '/tmp/autosklearn_tmp_64801_2482/.auto-sklearn/ensembles/1.0000000003.ensemble']\n"
     ]
    }
   ],
   "source": [
    "runtime = 30\n",
    "clf = AutoSklearn(runtime, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_autosklearn = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show which models the learner has picked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[(0.660000, SimpleClassificationPipeline({'balancing:strategy': 'none', 'categorical_encoding:__choice__': 'one_hot_encoding', 'classifier:__choice__': 'random_forest', 'imputation:strategy': 'mean', 'preprocessor:__choice__': 'no_preprocessing', 'rescaling:__choice__': 'standardize', 'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True', 'classifier:random_forest:bootstrap': 'True', 'classifier:random_forest:criterion': 'gini', 'classifier:random_forest:max_depth': 'None', 'classifier:random_forest:max_features': 0.5, 'classifier:random_forest:max_leaf_nodes': 'None', 'classifier:random_forest:min_impurity_decrease': 0.0, 'classifier:random_forest:min_samples_leaf': 1, 'classifier:random_forest:min_samples_split': 2, 'classifier:random_forest:min_weight_fraction_leaf': 0.0, 'classifier:random_forest:n_estimators': 100, 'categorical_encoding:one_hot_encoding:minimum_fraction': 0.01},\\ndataset_properties={\\n  'task': 2,\\n  'sparse': False,\\n  'multilabel': False,\\n  'multiclass': True,\\n  'target_type': 'classification',\\n  'signed': False})),\\n(0.340000, SimpleClassificationPipeline({'balancing:strategy': 'none', 'categorical_encoding:__choice__': 'one_hot_encoding', 'classifier:__choice__': 'random_forest', 'imputation:strategy': 'mean', 'preprocessor:__choice__': 'no_preprocessing', 'rescaling:__choice__': 'minmax', 'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'False', 'classifier:random_forest:bootstrap': 'True', 'classifier:random_forest:criterion': 'gini', 'classifier:random_forest:max_depth': 'None', 'classifier:random_forest:max_features': 0.9260795160807372, 'classifier:random_forest:max_leaf_nodes': 'None', 'classifier:random_forest:min_impurity_decrease': 0.0, 'classifier:random_forest:min_samples_leaf': 17, 'classifier:random_forest:min_samples_split': 7, 'classifier:random_forest:min_weight_fraction_leaf': 0.0, 'classifier:random_forest:n_estimators': 100},\\ndataset_properties={\\n  'task': 2,\\n  'sparse': False,\\n  'multilabel': False,\\n  'multiclass': True,\\n  'target_type': 'classification',\\n  'signed': False})),\\n]\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.show_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the error on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36055756843800324"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util.error(y_test, y_pred_autosklearn, 'classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: TPOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TPOT is an AutoML tool that optimizes machine learning pipelines by genetic programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run TPOT for 30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e37a2cefc44567812e6a465bdc53b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=20, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.5264128666666666 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: ExtraTreesClassifier(input_matrix, bootstrap=True, criterion=gini, max_features=0.9000000000000001, min_samples_leaf=1, min_samples_split=17, n_estimators=100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTClassifier(config_dict=None, crossover_rate=0.1, cv=5,\n",
       "               disable_update_check=False, early_stop=None, generations=1000000,\n",
       "               max_eval_time_mins=5, max_time_mins=0.5, memory=None,\n",
       "               mutation_rate=0.9, n_jobs=1, offspring_size=None,\n",
       "               periodic_checkpoint_folder=None, population_size=20,\n",
       "               random_state=None, scoring=None, subsample=1.0, template=None,\n",
       "               use_dask=False, verbosity=2, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2, max_time_mins=.5)\n",
    "tpot.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tpot = tpot.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the error on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3794949037476211"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util.error(y_test, y_pred_tpot, 'classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Oboe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oboe Example 1: build an ensemble of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experimental settings\n",
    "VERBOSE = False #whether to print out information indicating current fitting progress\n",
    "N_CORES = 1 #number of cores\n",
    "RUNTIME_BUDGET = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional: limit the types of algorithms\n",
    "s = ['AB', 'ExtraTrees', 'GNB', 'KNN', 'RF', 'DT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autolearner arguments\n",
    "autolearner_kwargs = {\n",
    "    'p_type': 'classification',\n",
    "    'runtime_limit': RUNTIME_BUDGET,\n",
    "    'verbose': VERBOSE,\n",
    "    'selection_method': 'min_variance',\n",
    "    'algorithms': s,\n",
    "    'stacking_alg': 'greedy',\n",
    "    'n_cores': N_CORES,\n",
    "    'build_ensemble': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intialize the autolearner class\n",
    "m = AutoLearner(**autolearner_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit autolearner on training set and record runtime\n",
    "start = time.time()\n",
    "m.fit(x_train, y_train)\n",
    "elapsed_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction error: 0.26755324989020646\n",
      "elapsed time: 20.5676531791687\n",
      "individual accuracies of selected models: [0.2690905888596106, 0.4200885668276973, 0.4208333333333333, 0.2690905888596106]\n"
     ]
    }
   ],
   "source": [
    "# use the fitted autolearner for prediction on test set\n",
    "y_predicted = m.predict(x_test)\n",
    "print(\"prediction error: {}\".format(util.error(y_test, y_predicted, 'classification')))\n",
    "print(\"elapsed time: {}\".format(elapsed_time))\n",
    "print(\"individual accuracies of selected models: {}\".format(m.get_model_accuracy(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ensemble method': 'greedy selection',\n",
       " 'base learners': {'GNB': [{}, {}],\n",
       "  'ExtraTrees': [{'min_samples_split': 0.1, 'criterion': 'gini'},\n",
       "   {'min_samples_split': 64, 'criterion': 'gini'}]}}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get names of the selected machine learning models\n",
    "m.get_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oboe Example 2: just select a collection of promising models without building an ensemble afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experimental settings\n",
    "VERBOSE = False #whether to print out information indicating current fitting progress\n",
    "N_CORES = 1 #number of cores\n",
    "RUNTIME_BUDGET = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional: limit the types of algorithms\n",
    "s = ['AB', 'ExtraTrees', 'GNB', 'KNN', 'RF', 'DT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autolearner arguments\n",
    "autolearner_kwargs = {\n",
    "    'p_type': 'classification',\n",
    "    'runtime_limit': RUNTIME_BUDGET,\n",
    "    'verbose': VERBOSE,\n",
    "    'selection_method': 'min_variance',\n",
    "    'algorithms': s,\n",
    "    'stacking_alg': 'greedy',\n",
    "    'n_cores': N_CORES,\n",
    "    'build_ensemble': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intialize the autolearner class\n",
    "m = AutoLearner(**autolearner_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit autolearner on training set and record runtime\n",
    "start = time.time()\n",
    "m.fit(x_train, y_train)\n",
    "elapsed_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 11.177161693572998\n",
      "accuracies of selected models: [0.3487545289855073, 0.3487545289855073, 0.3577999194847021, 0.280049590103938, 0.35440821256038646, 0.2487973027375201, 0.26432028619528614, 0.25504844642072905, 0.26432028619528614, 0.34665106682769725, 0.34665106682769725, 0.34665106682769725, 0.34665106682769725, 0.41362218196457323, 0.2690905888596106, 0.40555303945249593, 0.41857387278582936, 0.5, 0.41021286231884063, 0.2563190784658176, 0.40940633691992384, 0.41021286231884063, 0.2563190784658176, 0.3210897471087688, 0.3773461517347387, 0.3773461517347387, 0.3773461517347387, 0.3773461517347387, 0.27385014090177134, 0.1856392274191187, 0.27680015737080954, 0.27520632045088567, 0.2823746980676329, 0.26924933208900603, 0.2413370571658615, 0.4131514419557898, 0.3722901570048309, 0.37120571658615137, 0.28960849436392916, 0.386694847020934, 0.41977153784218996, 0.3922327898550725, 0.3927083333333333, 0.2823746980676329, 0.26924933208900603, 0.27385014090177134, 0.1856392274191187, 0.27385014090177134, 0.1856392274191187, 0.27385014090177134, 0.1856392274191187]\n"
     ]
    }
   ],
   "source": [
    "# use the fitted autolearner for prediction on test set\n",
    "y_predicted = m.predict(x_test)\n",
    " \n",
    "print(\"elapsed time: {}\".format(elapsed_time))\n",
    "print(\"accuracies of selected models: {}\".format(m.get_model_accuracy(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we do not have a single accuracy value here if we do not build an ensemble, instead, we just have a collection of fitted models with individual accuracies reported.\n",
    "\n",
    "The following shows which models we have picked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DT': [{'min_samples_split': 128},\n",
       "  {'min_samples_split': 128},\n",
       "  {'min_samples_split': 64},\n",
       "  {'min_samples_split': 32},\n",
       "  {'min_samples_split': 256},\n",
       "  {'min_samples_split': 16},\n",
       "  {'min_samples_split': 8},\n",
       "  {'min_samples_split': 4},\n",
       "  {'min_samples_split': 0.01},\n",
       "  {'min_samples_split': 0.001},\n",
       "  {'min_samples_split': 1e-05},\n",
       "  {'min_samples_split': 0.0001},\n",
       "  {'min_samples_split': 2},\n",
       "  {'min_samples_split': 512},\n",
       "  {'min_samples_split': 1024}],\n",
       " 'RF': [{'min_samples_split': 128, 'criterion': 'gini'},\n",
       "  {'min_samples_split': 2, 'criterion': 'gini'},\n",
       "  {'min_samples_split': 2, 'criterion': 'entropy'},\n",
       "  {'min_samples_split': 4, 'criterion': 'gini'},\n",
       "  {'min_samples_split': 4, 'criterion': 'entropy'},\n",
       "  {'min_samples_split': 8, 'criterion': 'gini'},\n",
       "  {'min_samples_split': 8, 'criterion': 'entropy'},\n",
       "  {'min_samples_split': 16, 'criterion': 'gini'},\n",
       "  {'min_samples_split': 16, 'criterion': 'entropy'},\n",
       "  {'min_samples_split': 32, 'criterion': 'gini'},\n",
       "  {'min_samples_split': 32, 'criterion': 'entropy'},\n",
       "  {'min_samples_split': 64, 'criterion': 'gini'},\n",
       "  {'min_samples_split': 64, 'criterion': 'entropy'},\n",
       "  {'min_samples_split': 128, 'criterion': 'entropy'},\n",
       "  {'min_samples_split': 0.1, 'criterion': 'gini'},\n",
       "  {'min_samples_split': 0.1, 'criterion': 'entropy'},\n",
       "  {'min_samples_split': 0.01, 'criterion': 'gini'},\n",
       "  {'min_samples_split': 0.01, 'criterion': 'entropy'},\n",
       "  {'min_samples_split': 0.001, 'criterion': 'gini'},\n",
       "  {'min_samples_split': 0.001, 'criterion': 'entropy'},\n",
       "  {'min_samples_split': 0.0001, 'criterion': 'gini'},\n",
       "  {'min_samples_split': 0.0001, 'criterion': 'entropy'},\n",
       "  {'min_samples_split': 1e-05, 'criterion': 'gini'},\n",
       "  {'min_samples_split': 1e-05, 'criterion': 'entropy'}],\n",
       " 'GNB': [{}],\n",
       " 'KNN': [{'n_neighbors': 1, 'p': 1}],\n",
       " 'AB': [{'n_estimators': 50, 'learning_rate': 1},\n",
       "  {'n_estimators': 50, 'learning_rate': 1.5},\n",
       "  {'n_estimators': 50, 'learning_rate': 2},\n",
       "  {'n_estimators': 100, 'learning_rate': 1},\n",
       "  {'n_estimators': 100, 'learning_rate': 1.5},\n",
       "  {'n_estimators': 100, 'learning_rate': 2}],\n",
       " 'ExtraTrees': [{'min_samples_split': 2, 'criterion': 'gini'},\n",
       "  {'min_samples_split': 0.001, 'criterion': 'gini'},\n",
       "  {'min_samples_split': 0.0001, 'criterion': 'gini'},\n",
       "  {'min_samples_split': 1e-05, 'criterion': 'gini'}]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.get_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
